{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e485ed-768d-49a6-9203-b4eb5c4f4363",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Deque, List, Optional\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import torch\n",
    "torch.set_num_threads(1)\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "\n",
    "# ==============================\n",
    "# ==============================\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "class DroneDelivery(gym.Env):\n",
    "\n",
    "    metadata = {'render.modes': []}\n",
    "\n",
    "    def __init__(self, size: int=6):\n",
    "        super().__init__()\n",
    "        self.max_step = 200\n",
    "        self.current_step = 0\n",
    "        self.reward = 0\n",
    "\n",
    "        self.size = size\n",
    "        self.num_control = 4\n",
    "\n",
    "        self.agent_location = np.array([-1,-1], dtype=np.int32)\n",
    "        self.pick_status = np.array([0], dtype=np.int32)\n",
    "        self.package_location = np.array([-1, -1], dtype=np.int32)\n",
    "        self.customer_location = np.array([-1,-1], dtype=np.int32)\n",
    "        self.no_fly_zone = np.array([-1, -1], dtype=np.int32)\n",
    "\n",
    "        self.observation = spaces.Dict(\n",
    "            {\n",
    "            \"agent\":spaces.Box(0, size-1, shape=(2,), dtype=int),\n",
    "            \"pick\":spaces.Box(0,1,shape=(1,), dtype=int),\n",
    "            \"package\":spaces.Box(0,size-1,shape=(2,), dtype=int),\n",
    "            \"customer\":spaces.Box(0, size-1, shape=(2,), dtype=int),\n",
    "            \"no_fly_zone\": spaces.Box(0, size - 1, shape=(2,), dtype=int),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        self.action_space = spaces.Discrete(self.num_control)\n",
    "\n",
    "        self._num_states = (self.size * self.size) * 2 * (self.size * self.size) * (self.size * self.size) * (\n",
    "                    self.size * self.size)\n",
    "\n",
    "        self.observation_space = spaces.Discrete(self._num_states)\n",
    "\n",
    "        self.action_to_direction = {\n",
    "            0: np.array([1,0]),  # Move right\n",
    "            1: np.array([0,1]),  # Move up\n",
    "            2: np.array([-1,0]), # Move left\n",
    "            3: np.array([0,-1]), # Move down\n",
    "        }\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return{\n",
    "            \"agent\": self.agent_location,\n",
    "            \"pick\": self.pick_status,\n",
    "            \"package\": self.package_location,\n",
    "            \"customer\": self.customer_location,\n",
    "            \"no_fly_zone\": self.no_fly_zone\n",
    "        }\n",
    "\n",
    "    def _get_info(self):\n",
    "        return {\n",
    "            \"distance\": np.linalg.norm(\n",
    "                self.agent_location - self.customer_location, ord=1\n",
    "            )\n",
    "        }\n",
    "\n",
    "    def obs_to_key(self, obs):\n",
    "        picked = int(obs[\"pick\"][0])\n",
    "        goal = obs[\"package\"] if picked == 0 else obs[\"customer\"]\n",
    "\n",
    "        dx = int(obs[\"agent\"][0] - goal[0])\n",
    "        dy = int(obs[\"agent\"][1] - goal[1])\n",
    "\n",
    "        m = self.size - 1\n",
    "        dx = max(-m, min(m, dx))\n",
    "        dy = max(-m, min(m, dy))\n",
    "\n",
    "        d_nfz = abs(obs[\"agent\"][0] - obs[\"no_fly_zone\"][0]) + abs(obs[\"agent\"][1] - obs[\"no_fly_zone\"][1])\n",
    "        nfz_bucket = 0 if d_nfz == 0 else 1 if d_nfz <= 1 else 2 if d_nfz <= 2 else 3\n",
    "        return (picked, dx, dy, nfz_bucket)\n",
    "\n",
    "    def reset(self,seed: Optional[int] = None, options: Optional[dict] = None):\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        self.reward = 0\n",
    "        self.current_step = 0\n",
    "\n",
    "        grid = np.array([[x,y] for x in range(self.size) for y in range(self.size)])\n",
    "        idx = np.random.choice(self.size*self.size, size=4, replace=False)\n",
    "\n",
    "        self.agent_location = grid[idx[0]]\n",
    "        self.pick_status = np.array([0], dtype=np.int32)\n",
    "        self.package_location = grid[idx[1]]\n",
    "        self.customer_location = grid[idx[2]]\n",
    "        self.no_fly_zone = grid[idx[3]]\n",
    "\n",
    "        self.state_grid = np.zeros((self.size, self.size))\n",
    "        self.state_grid[tuple(self.agent_location)] = 1\n",
    "        self.state_grid[tuple(self.package_location)] = 0.8\n",
    "        self.state_grid[tuple(self.customer_location)] = 0.6\n",
    "        self.state_grid[tuple(self.no_fly_zone)] = 0.4\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "        return observation, info\n",
    "\n",
    "    def step(self, action: int):\n",
    "        self.reward = 0\n",
    "        terminated = False\n",
    "\n",
    "        direction = self.action_to_direction[action]\n",
    "\n",
    "        self.agent_location = np.clip(self.agent_location + direction, 0, self.size - 1)\n",
    "\n",
    "        picked = np.array_equal(self.agent_location, self.package_location)\n",
    "        drop_off = np.array_equal(self.agent_location, self.customer_location)\n",
    "        in_no_fly_zone = np.array_equal(self.agent_location, self.no_fly_zone)\n",
    "\n",
    "        if picked and self.pick_status[0] == 0:\n",
    "            self.pick_status = [1]\n",
    "            self.package_location = np.array([-1, -1], dtype=np.int32)\n",
    "            self.reward += 25\n",
    "\n",
    "        self.reward += -100 if in_no_fly_zone else 0\n",
    "\n",
    "        if drop_off and self.pick_status[0] == 1:\n",
    "            self.pick_status = [0]\n",
    "            self.customer_location = np.array([-1, -1], dtype=np.int32)\n",
    "            terminated = True\n",
    "            self.reward += 100\n",
    "        else:\n",
    "            self.reward += -1  \n",
    "\n",
    "        self.current_step += 1\n",
    "        truncated = True if self.current_step == self.max_step else False\n",
    "\n",
    "        self.state_grid = np.zeros((self.size, self.size))\n",
    "        self.state_grid[tuple(self.agent_location)] = 1\n",
    "        self.state_grid[tuple(self.package_location)] = 0.8\n",
    "        self.state_grid[tuple(self.customer_location)] = 0.6\n",
    "        self.state_grid[tuple(self.no_fly_zone)] = 0.4\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "        reward = self.reward\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "    def render(self):\n",
    "        plt.title(\"Drone Delivery Environment\")\n",
    "        plt.imshow(self.state_grid)\n",
    "        plt.show()\n",
    "\n",
    "# ==============================\n",
    "# 2) DQN with Experience Replay\n",
    "# ==============================\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def safe_reset(env):\n",
    "    r = env.reset()\n",
    "    if isinstance(r, tuple) and len(r) == 2:\n",
    "        return r\n",
    "    return r, {}  \n",
    "\n",
    "def safe_step(env, action):\n",
    "    r = env.step(action)\n",
    "    if isinstance(r, tuple) and len(r) == 5:\n",
    "        return r\n",
    "    elif isinstance(r, tuple) and len(r) == 4:\n",
    "        obs, reward, done, info = r\n",
    "        return obs, reward, bool(done), False, info\n",
    "    else:\n",
    "        raise RuntimeError(\"Unsupported env.step return signature.\")\n",
    "\n",
    "def obs_to_state_vector(obs, env) -> np.ndarray:\n",
    "    picked, dx, dy, nfz_bucket = env.obs_to_key(obs)\n",
    "    m = max(1, env.size - 1)\n",
    "    return np.array([\n",
    "        float(picked),\n",
    "        float(dx) / float(m),\n",
    "        float(dy) / float(m),\n",
    "        float(nfz_bucket) / 3.0\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "@dataclass\n",
    "class Transition:\n",
    "    s: np.ndarray\n",
    "    a: int\n",
    "    r: float\n",
    "    s2: np.ndarray\n",
    "    done: bool\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity: int):\n",
    "        self.capacity = capacity\n",
    "        self.buffer: Deque[Transition] = deque(maxlen=capacity)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.buffer.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size: int) -> Transition:\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        s = np.stack([b.s for b in batch], axis=0)\n",
    "        a = np.array([b.a for b in batch], dtype=np.int64)\n",
    "        r = np.array([b.r for b in batch], dtype=np.float32)\n",
    "        s2 = np.stack([b.s2 for b in batch], axis=0)\n",
    "        d = np.array([b.done for b in batch], dtype=np.float32)\n",
    "        return Transition(s, a, r, s2, d)\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim: int, n_actions: int, hidden: Tuple[int, int] = (128, 128)):\n",
    "        super().__init__()\n",
    "        h1, h2 = hidden\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, h1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h1, h2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h2, n_actions)\n",
    "        )\n",
    "\n",
    "        def init_weights(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "        self.net.apply(init_weights)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "class EpsilonGreedy:\n",
    "    def __init__(self, start: float, end: float, decay: float):\n",
    "        self.eps = start\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.decay = decay\n",
    "\n",
    "    def value(self) -> float:\n",
    "        return self.eps\n",
    "\n",
    "    def step(self):\n",
    "        self.eps = max(self.end, self.eps * self.decay)\n",
    "\n",
    "def dqn_train(\n",
    "    env,\n",
    "    episodes: int = 1500,\n",
    "    buffer_size: int = 50_000,\n",
    "    batch_size: int = 64,\n",
    "    gamma: float = 0.99,\n",
    "    lr: float = 1e-3,\n",
    "    start_learning_after: int = 1_000,\n",
    "    learn_every: int = 4,\n",
    "    target_sync_every: int = 1_000,\n",
    "    eps_start: float = 1.0,\n",
    "    eps_end: float = 0.05,\n",
    "    eps_decay: float = 0.995,\n",
    "    max_grad_norm: float = 5.0,\n",
    "    seed: int = 0,\n",
    "    device: Optional[str] = None,\n",
    "):\n",
    "    set_seed(seed)\n",
    "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    n_actions = env.action_space.n\n",
    "    state_dim = 4  # [picked, dx_norm, dy_norm, nfz_norm]\n",
    "\n",
    "    online_q = QNetwork(state_dim, n_actions).to(device)\n",
    "    target_q = QNetwork(state_dim, n_actions).to(device)\n",
    "    target_q.load_state_dict(online_q.state_dict())\n",
    "    target_q.eval()\n",
    "\n",
    "    optimizer = optim.Adam(online_q.parameters(), lr=lr)\n",
    "    loss_fn = nn.SmoothL1Loss()  # Huber\n",
    "\n",
    "    buffer = ReplayBuffer(buffer_size)\n",
    "    eps_sched = EpsilonGreedy(eps_start, eps_end, eps_decay)\n",
    "\n",
    "    episode_returns: List[float] = []\n",
    "    epsilon_history: List[float] = []\n",
    "\n",
    "    global_step = 0\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        obs, _ = safe_reset(env)\n",
    "        s = obs_to_state_vector(obs, env)\n",
    "        ep_ret = 0.0\n",
    "        terminated = truncated = False\n",
    "\n",
    "        while not (terminated or truncated):\n",
    "            epsilon = eps_sched.value()\n",
    "            epsilon_history.append(epsilon)\n",
    "\n",
    "            if random.random() < epsilon:\n",
    "                a = env.action_space.sample()\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    qs = online_q(torch.from_numpy(s).unsqueeze(0).to(device))\n",
    "                    a = int(qs.argmax(dim=1).item())\n",
    "\n",
    "            obs2, r, terminated, truncated, _ = safe_step(env, a)\n",
    "            s2 = obs_to_state_vector(obs2, env)\n",
    "            done_flag = bool(terminated or truncated)\n",
    "\n",
    "            buffer.push(s, a, float(r), s2, done_flag)\n",
    "\n",
    "            ep_ret += float(r)\n",
    "            s = s2\n",
    "            global_step += 1\n",
    "\n",
    "            if len(buffer) >= max(batch_size, start_learning_after) and (global_step % learn_every == 0):\n",
    "                batch = buffer.sample(batch_size)\n",
    "\n",
    "                s_b = torch.from_numpy(batch.s).to(device)\n",
    "                a_b = torch.from_numpy(batch.a).to(device)\n",
    "                r_b = torch.from_numpy(batch.r).to(device)\n",
    "                s2_b = torch.from_numpy(batch.s2).to(device)\n",
    "                d_b = torch.from_numpy(batch.done).to(device)\n",
    "\n",
    "                # Q(s,a)\n",
    "                q_values = online_q(s_b).gather(1, a_b.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "                # 타깃: r + gamma*(1-d)*max_a' Q_target(s', a')\n",
    "                with torch.no_grad():\n",
    "                    next_q = target_q(s2_b).max(dim=1).values\n",
    "                    target = r_b + gamma * (1.0 - d_b) * next_q\n",
    "\n",
    "                loss = loss_fn(q_values, target)\n",
    "\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(online_q.parameters(), max_grad_norm)\n",
    "                optimizer.step()\n",
    "\n",
    "            if global_step % target_sync_every == 0:\n",
    "                target_q.load_state_dict(online_q.state_dict())\n",
    "\n",
    "        episode_returns.append(ep_ret)\n",
    "        eps_sched.step()\n",
    "\n",
    "        if (ep + 1) % 50 == 0:\n",
    "            mean100 = np.mean(episode_returns[-100:]) if len(episode_returns) >= 1 else ep_ret\n",
    "            print(f\"[Episode {ep+1:4d}] return={ep_ret:8.2f}  eps={eps_sched.value():.3f}  \"\n",
    "                  f\"mean100={mean100:8.2f}  buffer={len(buffer):6d}\")\n",
    "\n",
    "    target_q.load_state_dict(online_q.state_dict())\n",
    "\n",
    "    return {\n",
    "        \"online_q\": online_q,\n",
    "        \"target_q\": target_q,\n",
    "        \"episode_returns\": episode_returns,\n",
    "        \"epsilon_history\": epsilon_history,\n",
    "        \"device\": device\n",
    "    }\n",
    "\n",
    "# ==============================\n",
    "# ==============================\n",
    "\n",
    "@torch.no_grad()\n",
    "def select_greedy_action(online_q: QNetwork, s: np.ndarray, device: str) -> int:\n",
    "    qs = online_q(torch.from_numpy(s).unsqueeze(0).to(device))\n",
    "    return int(qs.argmax(dim=1).item())\n",
    "\n",
    "def evaluate_greedy(env, online_q: QNetwork, episodes: int = 20, device: str = \"cpu\", plot: bool = True):\n",
    "    rewards = []\n",
    "    for _ in range(episodes):\n",
    "        obs, _ = safe_reset(env)\n",
    "        s = obs_to_state_vector(obs, env)\n",
    "        terminated = truncated = False\n",
    "        ep_ret = 0.0\n",
    "        while not (terminated or truncated):\n",
    "            a = select_greedy_action(online_q, s, device)\n",
    "            obs2, r, terminated, truncated, _ = safe_step(env, a)\n",
    "            ep_ret += float(r)\n",
    "            s = obs_to_state_vector(obs2, env)\n",
    "        rewards.append(ep_ret)\n",
    "\n",
    "    print(\"\\n=== Greedy Evaluation (DQN) ===\")\n",
    "    print(\"Rewards:\", rewards)\n",
    "    print(f\"Mean: {np.mean(rewards):.3f} | Median: {np.median(rewards):.3f} | \"\n",
    "          f\"Best: {np.max(rewards):.3f} | Worst: {np.min(rewards):.3f}\")\n",
    "\n",
    "    if plot:\n",
    "        plt.figure()\n",
    "        plt.title(\"Greedy Evaluation: Total Reward per Episode (DQN)\")\n",
    "        plt.plot(range(1, len(rewards) + 1), rewards, marker=\"o\")\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Total Reward per Episode\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return rewards\n",
    "\n",
    "def run_one_greedy_episode_with_render_and_verify(env, online_q: QNetwork, device: str = \"cpu\", max_steps: int = 200):\n",
    "    def _tuple(a):\n",
    "        return tuple(int(x) for x in np.array(a).ravel())\n",
    "\n",
    "    obs, _ = safe_reset(env)\n",
    "    s = obs_to_state_vector(obs, env)\n",
    "\n",
    "    picked_once = False\n",
    "    delivered = False\n",
    "    terminated = truncated = False\n",
    "    total_reward = 0.0\n",
    "\n",
    "    print(\"\\n[Greedy Episode START]\")\n",
    "    if hasattr(env, \"render\"):\n",
    "        try:\n",
    "            env.render()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    step_idx = 0\n",
    "    while not (terminated or truncated):\n",
    "        a = select_greedy_action(online_q, s, device)\n",
    "        obs2, r, terminated, truncated, _ = safe_step(env, a)\n",
    "        total_reward += float(r)\n",
    "\n",
    "        print(f\"[Step {step_idx:03d}] action={a}, reward={r}\")\n",
    "        if hasattr(env, \"render\"):\n",
    "            try:\n",
    "                env.render()\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        try:\n",
    "            pk = int(obs2[\"pick\"][0])\n",
    "            if pk == 1:\n",
    "                picked_once = True\n",
    "            cust = _tuple(obs2[\"customer\"])\n",
    "            if cust == (-1, -1):\n",
    "                delivered = True\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        s = obs_to_state_vector(obs2, env)\n",
    "        step_idx += 1\n",
    "        if step_idx >= max_steps:\n",
    "            print(\"[WARN] max_steps reached, stopping.\")\n",
    "            break\n",
    "\n",
    "    print(f\"[Greedy Episode END] total_reward={total_reward:.1f}, terminated={terminated}, truncated={truncated}\")\n",
    "    ok = picked_once and delivered and (terminated or truncated)\n",
    "    if ok:\n",
    "        print(\"VERIFIED: Mission Complete.\")\n",
    "    else:\n",
    "        print(\"VERIFICATION FAILED:\",\n",
    "              f\"picked_once={picked_once}, delivered={delivered}, terminated={terminated}, truncated={truncated}\")\n",
    "    return ok, total_reward\n",
    "\n",
    "# ==============================\n",
    "# 4) Main\n",
    "# ==============================\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--episodes\", type=int, default=1500)\n",
    "    parser.add_argument(\"--seed\", type=int, default=0)\n",
    "    parser.add_argument(\"--buffer_size\", type=int, default=50000)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=64)\n",
    "    parser.add_argument(\"--gamma\", type=float, default=0.99)\n",
    "    parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
    "    parser.add_argument(\"--start_learning_after\", type=int, default=1000)\n",
    "    parser.add_argument(\"--learn_every\", type=int, default=4)\n",
    "    parser.add_argument(\"--target_sync_every\", type=int, default=1000)\n",
    "    parser.add_argument(\"--eps_start\", type=float, default=1.0)\n",
    "    parser.add_argument(\"--eps_end\", type=float, default=0.05)\n",
    "    parser.add_argument(\"--eps_decay\", type=float, default=0.995)\n",
    "    parser.add_argument(\"--max_grad_norm\", type=float, default=5.0)\n",
    "    parser.add_argument(\"--eval_episodes\", type=int, default=10)\n",
    "    parser.add_argument(\"--save_path\", type=str, default=\"dqn_drone_delivery.pt\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    set_seed(args.seed)\n",
    "\n",
    "    env = DroneDelivery()\n",
    "\n",
    "    results = dqn_train(\n",
    "        env,\n",
    "        episodes=args.episodes,\n",
    "        buffer_size=args.buffer_size,\n",
    "        batch_size=args.batch_size,\n",
    "        gamma=args.gamma,\n",
    "        lr=args.lr,\n",
    "        start_learning_after=args.start_learning_after,\n",
    "        learn_every=args.learn_every,\n",
    "        target_sync_every=args.target_sync_every,\n",
    "        eps_start=args.eps_start,\n",
    "        eps_end=args.eps_end,\n",
    "        eps_decay=args.eps_decay,\n",
    "        max_grad_norm=args.max_grad_norm,\n",
    "        seed=args.seed,\n",
    "    )\n",
    "\n",
    "    online_q = results[\"online_q\"]\n",
    "    device = results[\"device\"]\n",
    "    episode_returns = results[\"episode_returns\"]\n",
    "    epsilon_history = results[\"epsilon_history\"]\n",
    "\n",
    "    torch.save({\"state_dict\": online_q.state_dict()}, args.save_path)\n",
    "    print(f\"Saved DQN weights to: {args.save_path}\")\n",
    "\n",
    "    if len(episode_returns) > 0:\n",
    "        print(f\"\\nTrained episodes: {len(episode_returns)}\")\n",
    "        print(f\"Average return: {np.mean(episode_returns):.3f} | Median: {np.median(episode_returns):.3f}\")\n",
    "        plt.figure()\n",
    "        plt.title(\"DQN: Total Reward per Episode\")\n",
    "        plt.plot(episode_returns)\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Total Reward per Episode\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    if len(epsilon_history) > 0:\n",
    "        plt.figure()\n",
    "        plt.title(\"Epsilon Decay per Step\")\n",
    "        plt.plot(epsilon_history)\n",
    "        plt.xlabel(\"Step\")\n",
    "        plt.ylabel(\"Epsilon\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    evaluate_greedy(env, online_q, episodes=args.eval_episodes, device=device, plot=True)\n",
    "    run_one_greedy_episode_with_render_and_verify(env, online_q, device=device, max_steps=200)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
