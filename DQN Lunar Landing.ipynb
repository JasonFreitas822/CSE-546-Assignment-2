{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "519e221c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from time import time\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e131187c",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = gym.make(\"LunarLander-v3\", render_mode=\"human\")\n",
    "observation, info = environment.reset()\n",
    "\n",
    "for _ in range(1000):\n",
    "    action = environment.action_space.sample()  # Random action\n",
    "    observation, reward, terminated, truncated, info = environment.step(action)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = environment.reset()\n",
    "\n",
    "environment.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10d985b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the DQN, with the input being a list of states (numpy arrays)\n",
    "# we parameterize based on a sepcified sequence length\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, seq_len):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(8 * seq_len, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 4)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01ac0941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a preprocessing function\n",
    "\n",
    "def preprocess(s):\n",
    "    \n",
    "    # turn sequence of states into one array\n",
    "\n",
    "    s = np.concatenate(s) \n",
    "\n",
    "    # turn array to tensor\n",
    "\n",
    "    s = torch.tensor(s, dtype=torch.float32)\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f053cf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Deep Q-Learning Function\n",
    "\n",
    "def deep_Q_learning(sequence_length, init_epsilon, M, G, min_epsilon, decay, T, N, batch_size, gamma, C, epsiode_seeds):\n",
    "\n",
    "    # initialize replay memory, Q, Q hat and the initial epsilon\n",
    "\n",
    "    D = []\n",
    "    Q = Net(sequence_length)\n",
    "    Q_hat = Net(sequence_length)\n",
    "    epsilon = init_epsilon\n",
    "\n",
    "    # set the environment\n",
    "\n",
    "    environment = gym.make(\"LunarLander-v3\", render_mode = None)\n",
    "\n",
    "    # denote the loss function and optimizer\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(Q.parameters(), lr=0.0005)\n",
    "\n",
    "    # create lists to store total rewards for the episode and the epsilon value\n",
    "\n",
    "    REWARDS = []\n",
    "    DECAY = [epsilon]\n",
    "\n",
    "    # set a step count for global number of steps\n",
    "\n",
    "    step_count = 1\n",
    "\n",
    "    # iterate for M episode\n",
    "\n",
    "    for episode in range(M):\n",
    "\n",
    "        # adjust epsilon after G epsiodes (with minimum of min_epsilon)\n",
    "\n",
    "        if episode > G:\n",
    "            epsilon = max(min_epsilon, decay * epsilon)\n",
    "            DECAY.append(epsilon)\n",
    "        \n",
    "        elif episode > 0:\n",
    "            DECAY.append(epsilon)\n",
    "        \n",
    "        # initialize total reward\n",
    "\n",
    "        total_reward = 0\n",
    "        \n",
    "        # reset the environment, intialize the sequence and preprocessed sequence\n",
    "\n",
    "        x, _ = environment.reset()\n",
    "\n",
    "        s = [x for _ in range(sequence_length)]\n",
    "        phi = preprocess(s)\n",
    "\n",
    "        # initialize the termination term as False\n",
    "        # we will run every epsiode until termination\n",
    "\n",
    "        terminated = False\n",
    "\n",
    "        # iterate for (at most) T steps\n",
    "\n",
    "        for t in range(T):\n",
    "\n",
    "            # if the episode is done, end it\n",
    "            \n",
    "            if terminated:\n",
    "                break\n",
    "\n",
    "            # with probabilty epsilon, select a random action\n",
    "\n",
    "            if random.random() < epsilon:\n",
    "                a = environment.action_space.sample()\n",
    "            \n",
    "            # otherwise, pick the action with the best Q-value\n",
    "\n",
    "            else:\n",
    "                a = torch.argmax(Q(phi)).item()\n",
    "            \n",
    "            # take a step\n",
    "\n",
    "            x_prime, r, terminated, _, _ = environment.step(a)\n",
    "\n",
    "            # add termination penalty (if terminated) and adjust total reward\n",
    "\n",
    "            if terminated and total_reward < 200:\n",
    "                r -= 100\n",
    "\n",
    "            total_reward += r\n",
    "\n",
    "            # create new sequences\n",
    "\n",
    "            s_prime = s.copy()\n",
    "            s_prime.pop(0)\n",
    "            s_prime.append(x_prime)\n",
    "            phi_prime = preprocess(s_prime)\n",
    "\n",
    "            # get the episode-not-ended status as a binary variable\n",
    "\n",
    "            if terminated:\n",
    "                status = 0\n",
    "            \n",
    "            else:\n",
    "                status = 1\n",
    "\n",
    "            # store the transition in D (and remove a transition if needed)\n",
    "            # for y_j, we add the status\n",
    "\n",
    "            D.append((phi, a, r, phi_prime, status))\n",
    "\n",
    "            if len(D) >= N:\n",
    "                D.pop(0)\n",
    "            \n",
    "            # only perform gradient descent once we have enough for a full batch\n",
    "\n",
    "            if len(D) >= batch_size and episode > 100:\n",
    "\n",
    "                # get a minibatch from D\n",
    "\n",
    "                batch = random.sample(D, batch_size)\n",
    "\n",
    "                # get components of D as batches\n",
    "\n",
    "                PHI = torch.stack([datapoint[0] for datapoint in batch])\n",
    "                A = torch.tensor([datapoint[1] for datapoint in batch], dtype=torch.long)\n",
    "                R = torch.tensor([datapoint[2] for datapoint in batch], dtype=torch.float32)\n",
    "                PHI_PRIME = torch.stack([datapoint[3] for datapoint in batch])\n",
    "                STATUS = torch.tensor([datapoint[4] for datapoint in batch], dtype=torch.float32)\n",
    "\n",
    "                # get Q-values for Q under PHI and get predicitons\n",
    "\n",
    "                Q_values = Q(PHI)\n",
    "                predictions = Q_values.gather(1, A.unsqueeze(1)).view(-1)\n",
    "\n",
    "                # get the maximum of the Q_hat values under PHI_PRIME, and calculate y_j (targets)\n",
    "\n",
    "                Q_hat_values = Q_hat(PHI_PRIME).detach()\n",
    "                max_Q_hat = torch.max(Q_hat_values, dim = 1).values.detach()\n",
    "                targets = R + gamma * max_Q_hat * STATUS\n",
    "                targets[STATUS == 0] = R[STATUS == 0]\n",
    "\n",
    "                # get loss and update\n",
    "\n",
    "                loss = criterion(predictions, targets)\n",
    "                optimizer.zero_grad()\n",
    "                torch.nn.utils.clip_grad_norm_(Q.parameters(), max_norm=1.0)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            # adjust parameters of Q_hat every C epsiodes\n",
    "\n",
    "            step_count += 1 \n",
    "\n",
    "            if step_count % C == 0:\n",
    "                Q_hat.load_state_dict(Q.state_dict())\n",
    "            \n",
    "        # add total reward to REWARD\n",
    "\n",
    "        REWARDS.append(total_reward)\n",
    "\n",
    "    # return the model weights\n",
    "     \n",
    "    return Q.state_dict(), REWARDS, DECAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9beedb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untrained Total Rewards averaged out to -585.8915805986106 with a standard deviation of 170.81141104406305\n"
     ]
    }
   ],
   "source": [
    "# set the parameters\n",
    "\n",
    "sequence_length = 16\n",
    "init_epsilon = 1\n",
    "M = 20000\n",
    "G = 100\n",
    "min_epsilon = 0.01\n",
    "decay = 0.995\n",
    "T = 300\n",
    "N = 50000\n",
    "batch_size = 64\n",
    "gamma = 0.99\n",
    "C = 500\n",
    "\n",
    "# set the number of test episodes and seeds for each episode\n",
    "\n",
    "num_test_episodes = 100\n",
    "epsiode_seeds = [random.randint(0, 2**32 - 1) for _ in range(num_test_episodes)]\n",
    "\n",
    "# evaluate a random untrained model\n",
    "\n",
    "Q_untrained = Net(sequence_length)\n",
    "Q_untrained.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    untrained_total_rewards = []\n",
    "    environment = gym.make(\"LunarLander-v3\", render_mode = None)\n",
    "\n",
    "    for i in range(num_test_episodes):\n",
    "        observation, info = environment.reset(seed = epsiode_seeds[i])\n",
    "        s = [observation for _ in range(sequence_length)]\n",
    "        total_reward = 0\n",
    "\n",
    "        while True:\n",
    "            action = torch.argmax(Q_untrained(preprocess(s))).item()\n",
    "            observation, reward, terminated, truncated, info = environment.step(action)\n",
    "            s.pop(0)\n",
    "            s.append(observation)\n",
    "            total_reward += reward\n",
    "\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        \n",
    "        untrained_total_rewards.append(total_reward)\n",
    "    \n",
    "    environment.close()\n",
    "\n",
    "print(f\"Untrained Total Rewards averaged out to {np.mean(untrained_total_rewards)} with a standard deviation of {np.std(untrained_total_rewards, ddof=1)}\")\n",
    "\n",
    "# begin the training procedure\n",
    "\n",
    "training_start = time()\n",
    "\n",
    "weights, y_reward, y_decay = deep_Q_learning(sequence_length, init_epsilon, M, G, min_epsilon, decay, T, N, batch_size, gamma, C, epsiode_seeds)\n",
    "\n",
    "training_end = time()\n",
    "\n",
    "print(f\"Training took {(training_end - training_start) / 60} minutes\")\n",
    "\n",
    "# save the weights\n",
    "\n",
    "path = r\"C:\\Users\\jason\\OneDrive\\Desktop\\MY STUFF AS I WANT IT\\Python\\CSE 546 Assignment 2\"\n",
    "path_weights = os.path.join(path, \"a2_part_2_dqn_lunar_lander_Jason_Freitas_Kim_Kyudong.h5\")\n",
    "\n",
    "with h5py.File(path_weights, \"w\") as h5f:\n",
    "    for name, param in weights.items():\n",
    "        h5f.create_dataset(name, data=param.cpu().numpy())\n",
    "\n",
    "# evaluate the trained model\n",
    "\n",
    "Q_trained = Net(sequence_length)\n",
    "Q_trained.eval()\n",
    "\n",
    "with h5py.File(path_weights, \"r\") as h5f:\n",
    "    for name, param in Q_trained.named_parameters():\n",
    "        param.data.copy_(torch.tensor(h5f[name][:]))\n",
    "\n",
    "with torch.no_grad():\n",
    "    trained_total_rewards = []\n",
    "    environment = gym.make(\"LunarLander-v3\", render_mode = None)\n",
    "\n",
    "    for i in range(num_test_episodes):\n",
    "        observation, info = environment.reset(seed = epsiode_seeds[i])\n",
    "        s = [observation for _ in range(sequence_length)]\n",
    "        total_reward = 0\n",
    "\n",
    "        while True:\n",
    "            action = torch.argmax(Q_trained(preprocess(s))).item()\n",
    "            observation, reward, terminated, truncated, info = environment.step(action)\n",
    "            s.pop(0)\n",
    "            s.append(observation)\n",
    "            total_reward += reward\n",
    "\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        \n",
    "        trained_total_rewards.append(total_reward)\n",
    "    \n",
    "    environment.close()\n",
    "\n",
    "print(f\"Trained Total Rewards averaged out to {np.mean(trained_total_rewards)} with a standard deviation of {np.std(trained_total_rewards, ddof=1)}\")\n",
    "\n",
    "# plot the reward and epsilon\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(y_reward)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"Total Reward per Episode\")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(y_decay)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Epsilon\")\n",
    "plt.title(\"Epsilon Decay per Episode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4f1314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the testing rewards per epsiode\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(trained_total_rewards)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"Total Reward per Episode After Training\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
